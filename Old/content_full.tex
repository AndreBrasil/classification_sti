\section{Introduction}
\label{sec:multi:intro}

Data from the Web of Science reveal that Brazil and the Netherlands are very close regarding their indexed scientific output \autocite{Clarivate.2022}. Between 2017 and 2021, Brazil ranked 13th among the top-producing countries in the database, publishing a total of 289.562 papers and reviews. With 241.863 publications, The Netherlands followed in 14th place. Despite the proximity in absolute numbers, there are significant differences between the countries when results are observed from a relative perspective. For instance, while the Netherlands has a population of around 17.6 million \autocite{CBS.2021}, Brazil has already exceeded 213 million people \autocite{IBGE.2021}. That means the Latin American country produced 136 publications per 100.000 population, ten times less than the European counterpart, at 1374 publications. 

Another relevant distinction between the scientific production of the two countries is evident from the analysis of impact indicators. For example, considering the percentage of publications of each country among the upper 10\% percentile of the citation distribution in the same ﬁelds (PP top 10\%), Brazil performs below the average of the database, at 7,7\% of the expected 10\% value. The observed impact is significantly higher for The Netherlands, as 17,3\% of the country’s publications are in the top 10\% \autocite[see][]{Bornmann.2014}.

The differences in impact and relative productivity may suggest a higher level of efficiency exists in the Dutch science system, which has attracted the attention of Brazilian researchers \autocite[e.g.,][]{Marcovitch.2018, Verhine.2012}, policymakers \autocite[e.g.,][]{Barbosa.2020b}, and major funding organisations \autocite[e.g.,][]{CAPES.2018sc, Capes.2018sp}. In line with the views on research governance and the role of evaluation presented by \textcite{Molas-Gallart.2012}, Brazil seeks inspiration in the long-standing and stable Dutch evaluation system, which has been recognized as a key factor in the country’s quality assurance \autocite[17]{Drooge.2013}. 

While it is important for a country to investigate international experiences to improve its own practices, it is also necessary to cautiously reflect on potential learnings. On the one hand, Brazil already has one of the most sophisticated performance-based evaluation systems in the world, so any potential changes should consider what has already been achieved \autocite[5]{Faljoni-Alario.2018}. On the other hand, science systems can be as distinct as the social-economic circumstances, established governance, and cultural realities of each country. Potential disparities should be considered when trying to replicate any strategy that has been successful elsewhere. For instance, the design of the Brazilian science system must account for geographical realities in very particular ways, as the South American country, at 8.5 million km2, faces challenges that would be more comparable to the whole European Union – which is about half the size of Brazil, with 4.2 million km2 – than to The Netherlands, 205 times smaller at 41.543 km2 \autocite{CBS.2021, IBGE.2021}.

This work investigates the main differences and similarities in the research evaluation conducted in Brazil and The Netherlands. From the analysis of policy and guiding documents, connected legislation, and related literature, we compare the design of the science system in each country, examining their impact on the adopted evaluation models. For that, we adapt established comparison frameworks and focus the analyses on influential aspects for each system, such as the links between evaluation and funding, or the consequential effect the results can have on researchers’ behaviour. Finally, we conclude by highlighting inspiring methods and approaches from each evaluation system, so that those lessons could lead to positive change for both countries. 

\section{Methodology}
\label{sec:comp_evaluation:method}

According to \textcite{Galleron.2017}, different typologies of research evaluation systems have been proposed over time, but none of the existing frameworks was applicable for comparing multiple national systems, especially if disciplinary differences were to be considered. A suitable alternative has been investigated by a workgroup within the European Network for Research Evaluation in the Social Sciences and Humanities (ENRESSH), resulting in the design of a new comparison framework and accompanying codebook. This paper derives from that broader research project, which has supported analyses of national evaluation systems from 23 European countries, plus Brazil and South Africa \autocite{Ochsner.2020}.

Reproducing the effort behind the Brazilian report produced for ENRESSH by \textcite{Brasil.2022e}, this study begins with an extensive analysis of the practices and regulatory framework of the Dutch evaluation system. From the investigation of the Strategy Evaluation Protocol (SEP) active in the country \autocite{VSNU.2020}, as well as its previous iterations and related legislation, the study proceeds to apply an expanded version of the ENRESSH analytical categories to compare the national evaluation models in Brazil and the Netherlands. 

For each of the 19 categories in the comparison framework, the experiences from each country are contrasted, identifying distinctions and similarities either recorded in policy documents and connected legislation \autocite[e.g.,][]{OCW.1992, VSNU.2003, VSNU.2009, VSNU.2016}, or emerging from an extensive literature review conducted on national evaluations and evaluation impact, including the works by \textcite{Capano.2010}, \textcite{Hammarfelt.2015}, \textcite{Leeuw.2008}, \citeauthor{Molas-Gallart.2012} \autocite*{Molas-Gallart.2012, Molas-Gallart.2014}, \textcite{Ochsner.2021}, \textcite{Verhine.2012}, and others. As a result, we find not only the inspiration sought by Brazil, but we also identify pertinent lessons for The Netherlands.

\section{Results}
\label{sec:comp_evaluation:results}

The Dutch and Brazilian evaluation systems were developed from very different conceptions of assessment, university autonomy, and governance of higher education, science, and technology. Regulatory frameworks and improvements over time have institutionalized sui generis and unique systems, which seem to shape very distinct research cultures that may contribute to the previously mentioned impact differences.

\subsection{Organisational framework}
\label{subsec:comp_evaluation:organisation}



\input{tables/compeval_br_nl_organisation}

From the Brazilian side, the evaluation system was implemented in the 1970s and continues to evolve as a top-down, centralized model executed by a federal government organization: the Brazilian Agency for Support and Evaluation of Graduate Education (CAPES). The country’s graduate system is the result of a state policy that made the university the house of science in Brazil, mainly within master’s and doctoral courses. Evaluation guidelines, criteria, and procedures are, therefore, largely influenced by desired educational results. Additionally, CAPES accumulates the evaluation responsibility with two additional roles: regulating the graduate system and being the leading funding agency in the country. The combination of tasks and the strong links between evaluation results and funding makes the assessment model predominantly normative, standardized, and performance-based. Furthermore, the process seeks accountability through comparisons between graduate programs, leading to a high incidence of quantitative methods in the analyses, despite the ever-present participation of peer-review committees in the process \autocite{Brasil.2020, 977/1965, Verhine.2008}.

The Netherlands was the first country to institutionalise an evaluation system in Europe. In 1982, the Dutch model started being shaped as a bottom-up and decentralized process. While leading academic and scientific organizations in the country (VSNU, NWO, KNAW) design guiding policies, execution is based on the principle of autonomy and institutional planning. The research unit is the main unit of assessment, and those usually include PhD programs. Self-evaluation is the backbone of the system, contributing to a predominantly internal, formative, contextual, and qualitative assessment. Thus, units are evaluated based on their mission, aims, and strategies, in a self-knowledge practice not designed for external control, state regulation or accountability. Furthermore, results are not used to calculate the distribution of financial resources between institutions and research units, as there is a clear separation between assessment and financing \autocite{Drooge.2013, VSNU.2020}.


National  
The assessment is predominantly external and carried out at the national level. Master´s and doctoral courses are evaluated based on nationally defined criteria. HEIs don´t have the autonomy to create master and doctoral courses. Only the courses accredited by CAPES can be implemented. Furthermore, the permanence of the courses in the National System of Graduate Education (SNPG) depends on the results obtained in the quadrennial evaluation. 

In Brazil, the first higher education courses were implemented in the early 19th century. The courses were offered by small institutes and faculties with the purpose of promoting professional training (CUNHA, 1986; SAMPAIO, 1991, 2000). The first university was created in 1920 (University of Brazil). The universities promoted profound changes in higher education. They were directly responsible for the institutionalization of research. Until 1930, research was rare, incipient, restricted to a few areas and institutes (FÁVERO, 2006; MARTINS, 2018). The University of São Paulo (USP) was the first institution to implement a research-based university concept. It was created in 1934 to promote scientific knowledge and contribute to the country's development (GOLDEMBERG, 2015). Currently, the Brazilian higher education system is made up of public (federal, state, and municipal) and private (for profit  and nonprofit) institutions. Furthermore, there are at least three distinct types of institutions: universities, university centers, and faculties. In 2019, there were 2,608 higher education institutions (HEI), 8,603,824 enrollments in undergraduate courses, and 292.766 graduate students (MEC/INEP, 2019; CAPES, 2021a). Most HEIs are faculties. In 2019 there were 2,076 faculties, 294 university centers, 198 universities, and 40 federal institutes. About 88,4\% of the institutions were private. The private sector was responsible for 75,4\% 	of undergraduate enrollments and approximately 18\% of the graduate students (MEC/INEP, 2019; CAPES, 2021a). Public HEIs, on the other hand, offered 81.9\% of the graduate programs (master and doctorate courses) and were responsible for most of the research carried out in the country (CAPES, 2020).
The educational policies implemented in recent years have contributed to the expansion of higher education. In 1999, the country had 2,694,245 undergraduate students and 85,276 graduate students. Twenty years later, in 2019, there were 8,603,824 and 292,766 enrollments, respectively (MEC/INEP, 1999; MEC/INEP, 2019; CAPES, 2021a).  

Funding varies according to the legal nature of the HEIs. Private institutions essentially depend on the collection of tuition and fees. Furthermore, they receive complimentary resources from research calls and government student financing programs and grants for undergraduate and graduate students. Public HEIs, on the other hand, are basically funded by the State. They are free. Gratuity is a principle guaranteed by Art. 206 of the Federal Constitution (BRASIL, 1988). Students do not pay tuition. In 2019, public HEIs corresponded to 11.6\% of institutions (5.1\% state, 4.2\% federal, and 2.3\% municipal (MEC/INEP, 2019). 



The Dutch higher education system is made up of 14 research universities (known as Universiteiten-WO); 04 small special universities; dozens of universities of applied sciences in the fields of agriculture, fine arts, performing arts, education, etc. (known as Hogescholen-HBO); and an Open University (the Open University) (Drooge, 2021a; VSNU, 2022). Public institutions are maintained through public funding and resources from teaching, research and service provision (Goedegebuure, Westerheijden, 1991). 
 


The first graduate education regulatory framework was published in 1965. The Report 977/CFE/65 established the conceptual bases and general guidelines for the National System of Graduate Education organization (SNPG). The Report Sucupira, as it is known, can be considered the “founding document of systematic graduate education” in the country (CURY, 2005, p. 18). It created the legal and institutional conditions for expanding master and doctoral courses and for institutionalizing research. The graduate education was conceived as a policy to modernize the higher education system. In that context, graduate education was promoted with the aim of (i) training university professors and scientists, (ii) modernizing higher education, (iii) institutionalizing research into universities, and (iv) inserting the country into the international scientific community (BRASIL, 1965; MARTINS, 2018). For almost six decades, it has remained the principal doctrinal milestone of Brazilian graduate education (CURY, 2005, SAVIANI, 2020). The principles and guidelines established in 1965 have guided the organization and functioning of the SNPG. Based on Sucupira Report, the subsequent legislation introduced incremental changes in order to enhance the regulatory framework, to establish quality evaluation policies, and to promote the expansion of graduate education. Between 1965 and 2022, ten essential reports were published: 
 
1- Report 977/CFE/65 – established the conceptual bases and general guidelines for the organization of the SNPG (BRAZIL, 1965). 
2-	Report 77/CFE/69 – regulated the accreditation process for postgraduate programs (BRAZIL, 1969). 
3-	Decree nº 73.411/74 – created the National Council of Graduate Education and recommended the elaboration of the National Plan of Graduate Education (BRAZIL, 1974) 
4-	I National Plan for Graduate Education (I PNPG) – established policies for expansion and quality improvement for the period 1975-1979 (CAPES, 1975); 
5-	II PNPG – lay down guidelines for expansion and quality improvement for the period 1982-
1985 (CAPES, 1982); 
6-	III PNPG - established policies for expansion and quality improvement for the period 1986-
1989 (CAPES, 1986); 
7-	IV PNPG – lay down guidelines for development and quality improvement for the period 1990-2004 (CAPES, 1990); 
8-	V PNPG - established policies for expansion and quality improvement for the period 2005-
2010 (CAPES, 2004); 
9-	VI PNPG – lay down policies for expansion and quality improvement for the period 2011-
2020 (CAPES, 2011); 
10-	Proposal for improving the Graduate Education Model – this report recommended new guidelines for evaluation from 2021 on (CAPES, 2018). 

The Netherlands was the first European country to establish a quality teaching and teaching and learning evaluation system (WEERT, BOEZEROOY, 2007). The Conditional Financing (CF, 1982), published in 1982, can be considered the first regulatory framework to establish a formal quality assessment system in The Netherlands. Over the last four decades (1992-2022) eight important regulatory frameworks were published (Conditional Financing; Higher Education and Research Act; Protocol 1993-1998; Protocol 1998-2003; SEP 2003-2009; SEP 2009-2015; SEP 20152021 e SEP 2021-2026). The different reports have consolidated a stable evaluation and assurance quality system. The changes introduced over time were incremental and aimed at improving the system. There are clear lines of continuity between the regulatory frameworks.  

The protocols, known as "Strategy Evaluation Protocol (SEP), have been consolidated over time as the main regulatory frameworks for evaluation. According to the SEP in force (VSNU, NWO, KNAW, 2020, p. 06), the main goal of the protocols is to improve the quality and societal relevance of research and promote a continuous dialogue about actions that need to be implemented to increase transparency and accountability to society, funding institutions, and the government. In addition to establishing the guidelines, the protocols detail procedures to guide the evaluation. They are intended for all actors (people and institutions) who participate in the evaluation process, especially researchers, unit deans, university managers, board members, science and technology managers, committee members, committee secretaries, graduate education students, etc. Government agencies do not develop protocols. With the purpose to reduce interferences and external control to a minimum, in 1992, a substantial change was introduced in the legal framework that defines the competencies and attributions of the different institutions that make up the system. 
After the publication of the document Higher Education and Research Act (WHW, 1992), the Ministry of Education, Culture, and science stopped issuing guidelines and rules referring to the evaluation process. The regulation competencies started to be exercised by the associations representing the institutions that carry out the evaluated activities. The protocols began to be jointly elaborated by the three most important scientific and academic associations in the Netherlands: The Association of Dutch Universities (VSNU), the Netherlands Organisation for Scientific Research (NWO), and the Royal Netherlands Academy of Arts and Sciences (KNAW) (DROOGE, et al, 2013). The funding institutions should not be responsible for the evaluation. The separation between evaluation and funding is another critical feature of the Dutch model. The SEPs are reviewed every six years. The periodic reviews aim, among other objectives, to adapt the protocols to the needs of the different modalities of higher education and research institutions. For this reason, there isn't a single SEP in force in the country. There are specific protocols for research universities, applied sciences universities, and academic and non-academic research institutes (DROOGE, et al, 2013). 

Since the approval of the first regulatory framework in 1965, the Brazilian State has been the main policymaker, coordinator, and evaluator of the policies for the expansion, evaluation, and financing of graduate education. The leading role and the regulatory power held by the federal government have decisively contributed to the definition of the main characteristics of the National of Graduate Education (SNPG). Over the decades, the state regulation gave rise to a topdown, centralized, and centripetal organization model. The system organizes itself and reproduces itself from a central core. The center exercises regulatory power over the parts. It is responsible for producing the necessary rules for the system´s stability, regularity, coherence, and functioning. The parts relate to the whole in a dependent and hierarchical way (BARROSO, 2003). In the Brazilian SNPG, the regulatory role is performed by a federal public institution. Through the Brazilian Agency for Support and Evaluation of Graduate Education (CAPES), the Brazilian State designs, implements, and assesses graduate education development policies at national level. The CAPES is, in this sense, the principal federal agency in charge of designing, organizing, and carrying out the quality assessment. It continuously evaluates master´s and doctoral courses for accreditation and permanence in the system all around the country. Higher Education Institutions (HEI) don´t have the autonomy to create master´s and doctoral courses. Only the courses accredited by CAPES can be implemented. Furthermore, the permanence of the courses in the SNPG depends on the results obtained in the quadrennial evaluation. The courses are evaluated based on the criteria defined at national level. All institutions should meet the minimum quality standards. 

In Brazil, the evaluation is designed, organized, and performed by a federal public agency linked with the Education Ministry. The CAPES is responsible for both accreditation and quadrennial evaluation. All master’s and doctoral courses are evaluated based on previously defined criteria for each of the 49 fields of knowledge. The criteria for accreditation and periodic evaluation are distinct. They are prepared – and continually updated by the 49 coordinators of the fields of knowledge that make up the TechnicalScientific Council of CAPES. The guidelines, criteria and evaluation results are regularly published through the Field-Specific Documents, the Quadrennial Evaluation Regulations, legal ordinances, reports, etc. The CAPES is also responsible for the definition of evaluation timelines.  

The Dutch evaluation system is bottom-up and decentralized. There isn't a national institution – public or private – responsible for the conception, organization, and execution of the evaluation process. Evaluation policies are not defined by government, funding agencies, or private organizations. Regulatory frameworks are elaborated by country's three most important academic and scientific associations. The 
VSNU, NWO, and KNAW are autonomous and independent entities representing higher education institutions and the scientific community. Through protocols (Strategy Evaluation Protocol), published every six years, they establish guidelines that must guide all institutions' evaluation process. The coordination and the performance evaluation process are the responsibility of the institutions and evaluated units. The evaluation system was conceived and organized based on some fundamental principles over the decades. The evaluation must (i) be designed as an inherent dimension to the autonomy and strategic planning of institutions; (ii) be carried out by institutions and units that develop the academic activities; (iii) be carried out by peers independently, impartially, and transparently; (iv) combine internal and internal (self-assessment) and external (peer review) processes; (v) be conducted (carried out) without interference and control of the government and funding agencies; (vi) have a formative purpose, aimed at improving the quality of the evaluated units.  

In the Netherlands, there is not a national institution responsible for organizing, coordinating, and carrying out the evaluation process. Evaluation is conceived as a dimension closely linked to the principle of autonomy and institutional planning. The institutions and units are responsible for planning and performing all stages and requirements established by national protocols and internal quality evaluation policies. The institutions - through the Terms of Reference - define the attributions, procedures, guidelines, and ethical principles that should guide the evaluation process.  

In Brazil – differently from what happens in most countries – there are no specific regulatory frameworks for evaluating the quality of research at national level. Research is evaluated through graduate education. The interdependence and complementarity between these dimensions were institutionalized after the approval of the Sucupira Report in 1965. The Report 977/CFE/65 consolidated the principle that research is the main objective of graduate education. According to the 
Report (BRASIL, 1965, p. 164), “[…] the graduate education aims to offer, into the university, the environment, and available resources so that to perform the free scientific investigation […]”. The science and technology system has been structured around graduate education. Master’s and doctoral courses are responsible for the absolute majority of Brazilian scientific production (about 95\%) (BRASIL, 2020, 2021; BALBACHEVCKI, 2005; MARTINS, 2018, SCHWARTZMAN, 1989, 2001, 2005). For this reason, master's and doctoral courses are the two main assessment units. In Brazil –differently from what happens in Europe – the master’s courses are also evaluated. They are considered graduate studies, placed between undergraduate and doctorate degrees. The evaluation criteria are not the same for all course modalities. They vary according to the degree of the courses (academic master´s, professional master´s, academic doctorate, and professional doctorate). 

The protocols (SEP) do not establish separation between research and graduate education. For that reason, there are no specific regulatory frameworks for the evaluation of graduate education. The protocols establish guidelines for the assessment of research quality. Doctoral courses are conceived, organized, and evaluated within the scope of the research units. Master courses are not assessed by research units because - since the Bologna Accord – they have been part of the curricular structure of undergraduate courses. The evaluation units are, in general, research units. The institutions, however, have the autonomy to define the organization and scope. Evaluation can prioritize a single unit or to be carried out more broadly. It is possible to evaluate an area of knowledge at a national or institutional level, or research developed by a faculty, research center, or by a group of research units that work in a joint and integrated way ("umbrella" projects). Protocols use term unit to designate the research institutes, research centers, research groups, and multi and interdisciplinary research centers, etc. To be evaluated, the unit must meet the following conditions: (i) being recognized, internally and externally, as a research entity; (ii) having clear objectives, goals, and strategies shared by the group; (iii) having at least ten researchers FTEs among its permanent academic staff and (iv) having at least three years of operation (VSNU, NWO, KNAW, 
2020, p. 12 e 15).     

All master’s and doctoral courses are evaluated. In addition to initial evaluation (accreditation), the SNPG carries out periodic assessments. The first performance evaluation was carried out in 1976. Between 1976 and 1983, the assessment was annual. In 1984, it became biannual; in 1998, triannual and, in 2013, quadrennial (CAPES, 2013; VIANNA, 2018). The CAPES is responsible for preparing the evaluation calendar. All institutions and courses in the country must follow the same timeline.   

The evaluation protocols are reviewed every six years. The reviews aim to improve the regulatory frameworks and to establish new guidelines for the next cycle. The institutions also evaluate their units every six years. The evaluation is mandatory, but the protocols do not set rigid timelines. There isn’t a unified national calendar. The institutions have the autonomy to define the timeline. The evaluation is carried out according to the planning of the institutions and of each evaluated unit.      

\subsection{Methods and data}
\label{subsec:comp_evaluation:methods}

Over the decades, Brazil consolidated a complex, sophisticated, and stable evaluation system. In 1965, the Report 977/65 institutionalized the peer review system and became an external, independent, and impartial mandatory evaluation. All accreditation processes for new courses and periodic evaluation are analyzed by experts and ad hoc consultants from 49 fields of knowledge. The CAPES carries out the coordination of all stages of the external evaluation. In the last quadrennial assessment, which was concluded in 2017, approximately 6,303 master’s and doctoral courses were evaluated. The process involved around 2,000 panel members (BRASIL, 2021). Evaluation is essentially based on a peer review system. The external evaluation prevails over the internal one. Selfevaluation is a little institutionalized practice in graduate programs. Its results are not incorporated by the external evaluation.            

The system combines internal and external evaluation. The steps are complementary. The first is carried out through self-evaluation and aims to strengthen the autonomy and institutional development. The second is conducted by external experts (preferably foreigners) and intends to promote impartial and independent analyses of the unit. Each institution defines the criteria e procedures for choosing the members of the external committee. A specialized and independent secretariat supports the committee´s activities. All committee members must sign an impartiality statement. Internal evaluation precedes and guides external evaluation. The external committee analyses the self-assessment report prepared by the unit. The analysis criteria are not standardized and homogenous. The assessment is not regulatory. It does not aim to set rankings and to create indicators of comparability among units.  

\input{tables/compeval_br_nl_methods}

The organization and modus operandi of the system were established based on the principle that evaluation should be external, independent, and carried out by peers (informed peer review). The regulatory frameworks, organization, and evaluation procedures are defined through a complex system of cooperation, dialogue, and division of the competencies between the main statutory agency and the scientific community. CAPES is responsible for the metaevaluation and macro-efficiency of the SNPG. The quality evaluation, on the other hand, is entirely carried out by experts and ad doc consultants from the 49 fields of knowledge. The analysis of the projects to create new courses (accreditation) is carried out exante. The periodic evaluation takes place every four years (ex-post). Since 1998, the evaluation results have been expressed through grades (1 to 7). Grades 1 and 2 have been attributed to courses that do not meet the minimum requirements. Grades 6 and 7, on the other hand, are destined for courses that have high-quality standards CAPES 2019, 2021). Grades enable comparisons among courses, institutions, and fields of knowledge. They are used to emphasize the quality of courses and calculate the financial resources. 

Dimensions and evaluation criteria are defined and improved periodically. The Technical-Scientific Council (CTS-ES) is responsible for preparing e approving the “Evaluation Form”. The CTC-ES is comprised of dozens of members, mainly 49 coordinators of fields of knowledge. The evaluation form sets out standards and mandatory criteria for all fields and courses. Both evaluation processes – the periodic one and the one for accreditation of new courses – must consider the requirements of the evaluation form and the guidelines established by the documents of each field of knowledge. The first standardized form was elaborated in 1998. In 2007, the CTC-ES decided to reduce the number of requirements (from 7 to 5). In 2013, a new review was carried out. In 2018, the number of requirements and items was reduced again. The current form includes only three criteria (Program, Training, and Impact on society) and 12 items (MONTEIRO, et al. 2019). Through these criteria, it is intended to analyze, among other items, (i) the quality of training and scientific production of professors and students; (ii) internationalization; (iii) innovation and knowledge transfer; and (iv) the economic, social and cultural impacts on the society (CAPES, 2018). 


Self-evaluation is the backbone of the system. Institutions and units are responsible for organization and management of all stages of the process. Evaluation is a self-management practice. The core of the evaluation are the objectives, goals, and strategies that the unit itself establishes to improve quality, societal relevance, and viability (VSNU, NWO, KNAW, 2020; DROOGE, 2021b). The cycle comprises a period of six years, and it is composed of at least eight phases: (i) definition of the unit's strategies, objectives, and goals; (ii) elaboration the Terms of Reference for the evaluation; (iii) preparation of the self-evaluation report by the evaluated unit; (iv) selection and nomination of the members of the external evaluation committee; (v) site visit by the external committee; (vi) elaboration of the final report of the external committee; (vii) publication of the final position document, and (viii) discussion of results and follow-up.     

Self-evaluation (AA) is the backbone and the central feature of the Dutch system. It articulates the principles, objectives, criteria, and modus operandi of the evaluation process. According to the SEP  20212027 (VSNU, NWO, KNAW, 2020, p. 19), the AA allows a reflection […] on the strategic choices that the unit has made as well as the effects that these choices have had.” The unit analyses the results reached in light of its own aims and strategy through it. 
According to Drooge (2021b, p. 2), the evaluation 
“[...] is not so much focused on the research itself, as it is on the strategy of the unit with regards to research”. The unit is both the starting point and endpoint of the evaluation process. The final results return to the unit – in the form of a report – in order to guide the strategic planning of the following evaluation cycle. The AA promotes institutional development and holds the unit accountable for continuous improvement. 


In Brazil, the evaluation is predominantly external, standardized, and quantitative. Over the decades, the national policies have not assigned a relevant role of internal assessment. For this reason, the self-evaluation does not integrate the evaluation process performed at national level. The lack of guidelines has discouraged courses from institutionalizing permanent self-evaluation policies and practices. This absence has been pointed out as one of the main gaps in the SNPG 
(CAPES, 2018; 2019, 2020, FOPROP, 2018, FALJONI-ALARIO, 2018). The external evaluation has been essential to promoting quality in a continental country like Brazil. Despite that, it has some shortcomings. In a recent report (CAPES, 2018), the Technical Scientific Council recognized this weakness and recommended self-evaluation to be an institutionalized, permanent and participatory process. 
According to this document, self-evaluation must “[…] be carried out through participatory processes, based on various strategies, techniques, and instruments, generating analytical reports that point out the program´s strengths and weakness and reveal policies and actions for correction and consolidation”. 
(CAPES, 2018, p. 19). 



The dimensions and evaluation criteria have been improved over the decades. The most significant changes were introduced in 2009 and 2015. The SEP 2009-2015 introduced an important change to the word "relevance". The term came to be called "societal relevance". In addition to scientific impact, the evaluation should consider contributions from research to the development of the other areas as economy, innovation, culture, public management, etc. (VSNU, NWO, KNAW, 2009; DROOGE, et al, 2013). In 2015, the SEP 2015-2021 reduced the number of dimensions 
(from 4 to 3). The term "productivity" was excluded (VSNU, NWO, KNAW, 2014). Since this change, evaluation started to take into account three dimensions: quality, societal relevance, and viability. The protocols suggest specific criteria for each of the dimensions. However, the criteria are not rigid and standardized. The institutions and evaluation committees have the autonomy to make adjustments according to the characteristics of each evaluated unit. In addition, the criteria mix qualitative and quantitative dimensions. The purpose is not t to set rankings and indicators of comparability. The evaluation results do not allow comparisons between units (DROOGE, et al, 2013; VSNU, NWO, KNAW, 2020). 

The site visit was implemented in the early 1980s. A few years later, however, it was no longer mandatory. Currently, it is performed when there is a recommendation from the Technical Scientific 
Council.   

The site visit is an important stage of the evaluation process. It takes place a few months after the unit has concluded and delivered the self-evaluation report. Through the site visit – which lasts between one or two days – the external evaluation committee has the opportunity to (i) assess the research infrastructure; (ii) to get to know the researchers and members of the unit; (iii) to hold meetings and interviews with management, researchers, staff, PhD candidates and stakeholders and, (iv) to request additional information and documents (VSNU, NWO, KNAW, 2020, p. 13, 21, 42). At the end of the visit, the external committee usually presents the main conclusions, insights, and recommendations. The site visit offers elements to elaborate the final report evaluation. 

\subsection{Evaluation stakes}
\label{subsec:comp_evaluation:funding}

Evaluation is the backbone of the SNPG. Since 196, it has played a strategic role within the system. Periodic evaluations have generated a solid, reliable, and updated set out of evidence and indicators about graduate education and research in the country. The results have contributed decisively to the definition of quality improvement policies. Brazilian graduate education is nationally and internationally recognized for its high level of academic and organizational quality BALBACHEVCKI, 2005; BARRETO; DOMINGUES, BORGES, 2014; CAPES, 2018; MARTINS, 2018; SAVIANI, 2000, 2013, 2020; VERHINE; DANTAS, 2009). In Brazil, the evaluation results have produced consequences. The deployments, however, are known and legitimated by the academic community and by graduate education programs. Both evaluators and those evaluated know and agree with the modus operandi and the consequences of the evaluation process. The actor´s expectations are stabilized. After almost sixty decades of existence, the system has achieved a high level of reliability and transparency. The results trigger several consequences within the system, especially on the courses evaluated. The evaluation defines, among other aspects, (i) the accreditation of new courses; (ii) the grades of existing courses; (iii) the number of scholarships; (iv) the number of funding resources and (v) the access to special financing programs, especially research and internationalization calls.  

The protocols do not define rules and guidelines on consequences of the evaluation results. The regulatory frameworks do not set relations between performance and funding. Institutions manage sanctions, rewards, and incentives according to their internal policies. The main institutional mechanism of quality assurance is the follow-up. Through it, the institutions – jointly with the units – define and monitor the strategic actions which should be implemented. The institutions may, when necessary, to recommend carrying out midterm assessments (VSNU, NWO, KNAW, 2020). As the evaluation is decentralized, the system does not provide an overview of the research quality in the country. Furthermore, it is not possible to know which measures were implemented by institutions intended to remedy the deficiencies pointed out by the external committee (DROOGE, et al, 2013).  

\input{tables/compeval_br_nl_stakes}


The primary purpose of the Brazilian evaluation system is to improve graduate education quality at national level. The assessment is, however, normative and regulatory. It does not have a formative purpose. It is carried out to verify whether the graduate programs and knowledge areas meet the minimum quality 
standards defined at national level. It is a performance-
based system. The evaluation emphasizes the regulation, control, efficiency, productivity, accountability, rankings, and comparisons among institutions, courses, and knowledge areas. According to the 7th Article of the Quadrennial Assessment Regulation (CAPES, 2021), the evaluation aims: “[…] I- portraying the situation of Brazilian graduate education in the quadrennium; II- assessing the performance of postgraduate programs; III- ensuring quality; IV- evaluating the training of masters and doctors; V-  analyzing the intellectual production of the graduate education programs and its social, economic and cultural impact, and VI- contributing for the evolution and improvement of the Brazilian graduate education, recognizing the different stages of development of the diverse fields of knowledge and the regional asymmetries […]”. The external appraisal is, in this sense, standardized, homogeneous, quantitative, and comparative. Through it, the Brazilian State (i) assesses the performance of institutions and courses; (ii) monitors the quality at the national level; (iii) establishes rankings and comparisons, and (iv) evaluates the effectiveness and the results of implemented policies and, (v) sets out improvements in the system (DIAS SOBRINHO, 
2004; VERHINE, FREITAS, 2012; VERHINE, 2008).  


The general objectives of the evaluation were defined by the Higher Education and Research Act (WHW, 1992). The protocols published from 1993 onwards consolidated the principle that assessment should promote permanent quality improvement. According to the Strategy Evaluation Protocol (SEP 2021-2027), the main objective of the evaluation "[…] is to maintain and improve the quality and societal relevance of research as well as to facilitate continuous dialogue about research quality, societal relevance and viability in the context of research quality assurance (VSNU, NWO, KNAW, 2020, p. 6). The evaluation is contextual and essentially formative, i.e., it is carried out by the institutions and units themselves with the purposes of analyzing the results, identifying the strengths, weaknesses and defining the changes and improvements that need to be implemented (VSNU, NWO, KNAW, 2020, DROOGE, 2021a, 2021b, 
2021c, 2021d). It is designed and carried out as a practice of self-government, self-management, selfknowledge and institutional planning. The evaluation process aims, in this sense, to preserve and strengthen the values and purposes of academic activity, especially the autonomy, academic freedom, scientific quality, societal relevance, transparency, and the participation of the academic community. Evaluation is not, in a sense, an instrument of external control, state regulation, accountability, and performance analysis. It is not intended to establish rankings and comparisons between units or offer indicators for the distribution of rewards and awards, especially grants and financial resources. It is an internal, procedural,  participatory practice carried out based on each evaluated unit's identity, objectives, and strategies of each assessed unit. The central purpose of the evaluation is to improve the quality, enhance societal relevance, increase transparency and purpose improvements (VSNU, NWO, KNAW, 2020; WEERT; BOEZEROOY, 2007; VERHINE, 
FREITAS, 2012; VUGHT, WESTERHEIJDEN, 1993; AMARAL, 1997, BRENNAN, SHAH, 2000. The protocols define the general evaluation objectives at the national level, but the institutions have the autonomy to reorganize them according to their needs. The goals are not rigid, standardized, and mandatory for all institutions in the country. Besides that, there is not nationally defined rules regarding the consequences of assessment. It is up to the institutions to determine what to do with the results. No national institution is responsible for defining and applying rewards and sanctions (DROOGE et al, 2013).    


Evaluation and funding are two central pillars of the SNPG. Since 1065, these two dimensions have been conceived and performed as public policies to improve quality and foster graduate education courses. The evaluation generates grades and rankings that enable to compare the performance by fields of knowledge. The results are used as criteria to calculate the distribution of the resources among institutions, courses, and researchers. It is a performance-based system. According to the adapted quality and efficiency criteria, the best-evaluated courses receive more grants and resources for funding and research infrastructure. The financing is a kind of award and reward. In addition, the design and management of evaluation and funding policies are performed by the same institution. The CAPES is both a financing and evaluation agency. It establishes the guidelines and goals to be reached and, at the same time, monitors the results achieved. (BARROSO, 2005; (FERREIRA, MOREIRA, 2002; MARTINS, 2018; HICKS, 2011; BRASIL, 2020). 

The separation between assessment and funding is one of the main characteristics of the Dutch system. The universities and the research are mostly financed with public resources, but the State is not responsible for designing, organizing, and carrying out the evaluation. Unlike in other national systems, in the Netherlands, the State doesn't act as an evaluator. The Higher Education and Research Act (WHW, 1992) initially proposed this division of the competencies in 1992. The protocols published over thirty years consolidated the principle that peers must evaluate science based on the assumptions and criteria that guide the academic ethos. The funding institutions should not be responsible for the evaluation. Furthermore, the country has not implanted the "performance-based university research funding system" (HICKS, 2011; OCHSNER, et al, 2018). The assessment is not an instrument of control and accountability. The results are not used to distribute funding between institutions and units (MOLAS-GALLART, 2012). 


The Brazilian system is top-down, centralized, homogeneous, and centripetal. Assessment is predominantly external, regulatory, standardized, and performance-based. It is a public policy that aims to establish guidelines, criteria, procedures, and minimum performance standards for all institutions and graduate programs. The evaluation results have been used to define funding, transparency, and quality improvement quality. With such objectives and characteristics, the system has, on the one hand, promoted control and homogenization and, on the other hand, has reduced the differentiation, diversity, and institutional autonomy (ESTRELA, SIMÃO, 2003; VERHINE, FREITAS, 2012; FREITAS, 2012; (MOLAS-GALLART, 2012).     

The Dutch system is decentralized and heterogeneous. The units are evaluated based on their own mission, aims, and strategies. The evaluation respects differences and specificities. Instead of standardizing and homogenizing the system, the evaluation process preserves diversity and promotes differentiation. Therefore, criteria are not homogeneous and do not generate standardized results for the entire system. The evaluation mixes quantitative and qualitative criteria. The purpose is not quantifying and comparing the performance of different units. The process does not intend to produce quantitative data for the government and funding agencies.          (VSNU, NWO, KNAW, 2020; DROOGE et al., 2013; ESTRELA, SIMÃO, 2003; VERHINE, FREITAS, 2012).   


\subsection{Transparency and controversies}
\label{subsec:comp_evaluation:transparency}



\input{tables/compeval_br_nl_stability}

Transparency is one of the main features of the SNPG. All guidelines, criteria, results, and information about evaluation are regularly published on the CAPES website, especially the evaluation form, field-specific documents, quadrennial evaluation regulation, and the final results of the assessment. In addition, there are two robust coleta systems for collecting and managing research and graduate education (GeoCapes and Sucupira Platform). The GeoCapes offers updated information on graduate programs across the country (courses, enrollments, professors, students, grades, funding, international cooperation, etc.). It is a valuable accountability tool. Users can access graduate education data since 1995. The second system also offers multiple functionalities. The Platform Sucupira is an online tool for collecting and updating information on each of the graduate programs. Through it, courses organize and send their information to CAPES. The quadrennial evaluation is carried out based on the data available on the Sucupira 
Platform (BRASIL, 2022; CAPES, 2022a, 2022b).         

The academic community has recognized the importance and strategic role of the evaluation. Brazilian graduate education has reached a high level of academic quality and organization. In addition, the evaluation policies have decisively contributed to the development and qualification of Brazilian science (BALBACHEVCKI, 2005; CAPES, 2018; MARTINS, 2018; SAVIANI, 2000, 2013, 2020; VERHINE; 
DANTAS, 2009). According to the Web of Science, 
Brazil currently occupies the 13th position among topproducing countries (289.562 indexed publications between 2017-2021). Despite this, the scientific community has pointed out several gaps and flaws in the evaluation process. The main criticisms were presented after the end of the quadrennial assessment (2013-2016). During 2017 and 2018, the Special Monitoring Committee of the National Postgraduate Plan (PNPG Committee) coordinated a broad set of analyses on the evaluation model. The main scientific associations and regulatory and funding agencies were consulted.  The final report of the committee – approved by CAPES Superior Council in 2018 – was very empathetic about the need to rethink the system. According to the report, “[...] the current evaluation system has reached a point of exhaustion and should be conceptually and objectively rethought and improved” (CAPES, 2018. p. 3). Based on this diagnosis, the report recommended, among other aspects, (i) the adoption of a multidimensional evaluation model, (ii)  the institutionalization of selfassessment, (iii) a better balance between the quantitative and qualitative dimensions of evaluation, (iv) monitoring graduates; (v) reducing regional asymmetries and (vi) strengthening of innovation and the impact of graduate education on society. 


\section{Potential lessons}
\label{sec:comp_evaluation:discussion}

What can both countries learn from each other?

\subsection{Data quality and information systems}
\label{subsec:comp_evaluation:cris}



\section{Conclusions}
\label{sec:comp_evaluation:conclusions}

One of the central conclusions of the present work comes from the distinct relationship between evaluation and funding in Brazil and The Netherlands. Performance-based funding systems not only influence researchers’ way of working but also limit the design of the evaluation model that can be implemented \autocite{Hicks.2012}. In Brazil, the high-stakes evaluation model impacts funding and even the accreditation of the units assessed, which is not renewed in case of poor performance. Trust is an issue, and assessment is seen as an audit procedure, making reliance on quantitative indicators very high \autocite{Rafols.2016}. That means the Dutch formative evaluation, based on self-assessment practices, would face significant obstacles to work in Brazil without a reorientation of evaluation goals. Furthermore, even the periodicity of evaluation is impacted by the connection between assessment results and funding, as longer cycles represent fewer opportunities for the Brazilian graduate programs to get better results and the additional resources that derive from the achievement.

At the same time, the Brazilian high-stakes evaluation system has also led to a series of positive developments that could inspire the evolution of the Dutch model, or the tools in existence to support it. For instance, Brazil has developed very advanced current research information systems designed to support evaluation with bibliometric and scientometric data \autocite{Siqueira.2019}. Data is collected from graduate programs, higher education institutions, funding agencies and more, being open to the public and for research purposes \autocite{CAPES.2021d}. A centralized effort to collect, validate, clean, and generate comparable evaluation data for the whole research system could support the self-assessment of Dutch research units in very positive ways.

A series of additional analyses are included in the full paper, making it clear that the peculiarities of each evaluation system are strongly rooted in the core decisions behind each of those systems. No analytical category in the comparative study can be interpreted in isolation as each of them have dependencies and consequences, creating an interconnected mesh that can’t be simply unmade.
