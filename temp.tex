

i)	The protocols published from 1993 onwards consolidated the principle that assessment should promote permanent quality improvement. >>

More comprehensive objectives for the evaluation were established by the Higher Education and Research Act \autocite{OCW.1992}. The works by \textcite{Goedegebuure.1991, Drooge.2013, Weert.2007} show the Dutch system was conceived and organized based on some fundamental principles, stating the evaluation must (i) be designed as an inherent dimension to the autonomy and strategic planning of institutions; (ii) be carried out by institutions and units that develop the academic activities; (iii) be carried out by peers independently, impartially, and transparently; (iv) combine internal and internal (selfassessment) and external (peer review) processes; (v) be conducted (carried out) without interference and control of the government and funding agencies; (vi) have a formative purpose, aimed at improving the quality of the evaluated units.


It is designed and carried out as a practice of self-government, self-management, selfknowledge and institutional planning. T

Results have contributed decisively to the definition of quality improvement policies. Results trigger several consequences within the system, especially on the courses evaluated. The evaluation defines, among other aspects, (i) the accreditation of new courses; (ii) the grades of existing courses; (iii) the number of scholarships; (iv) the number of funding resources and (v) the access to special financing programs, especially research and internationalization calls.

According to the SEP in force \autocite[6]{VSNU.2020}, the main goal of the protocols is to improve the quality and societal relevance of research and promote a continuous dialogue about actions that need to be implemented to increase transparency and accountability to society, funding institutions, and the government.

	According to the 7th Article of the Quadrennial Assessment Regulation \autocite{122/2021}, the evaluation aims: “\textelp{} I- portraying the situation of Brazilian graduate education in the quadrennium; II - assessing the performance of postgraduate programs; III- ensuring quality; IV- evaluating the training of masters and doctors; V- analyzing the intellectual production of the graduate education programs and its social, economic and cultural impact, and VI- contributing for the evolution and improvement of the Brazilian graduate
education, recognizing the different stages of development of the diverse fields of knowledge and the regional asymmetries \textelp{}”.

(a)	The protocols, known as "Strategy Evaluation Protocol (SEP), have been consolidated over time as the main regulatory frameworks for evaluation. Through protocols (Strategy Evaluation Protocol), published every six years, they establish guidelines that must guide all institutions' evaluation process.  (DROOGE et al., 2013; DROOGE et al., 2022; (VSNU, KNAW, NWO, 2003, 2009, 2014, 2020). >>

i)	 >>

(A)	The protocols published over thirty years consolidated the principle that peers must evaluate science based on the assumptions and criteria that guide the academic ethos. >>



(2)	The institutions and units are responsible for planning and performing all stages and requirements established by national protocols and internal quality evaluation policies. (Statement based on the protocols, especially (VSNU, NWO, KNAW, 2020) >>


CAPES carries out the coordination of all stages of the external evaluation. In the last quadrennial assessment, which was concluded in 2017, 6,303 master’s and doctoral courses were evaluated. The process involved around 2,000 panel members \autocite{CAPES.2018}. Dimensions and evaluation criteria are defined and improved periodically. The Technical-Scientific Council (CTS-ES) is responsible for preparing e approving the “Evaluation Form”.

Regulatory frameworks, organization, and evaluation procedures are defined through a complex system of cooperation, dialogue, and division of the competencies between the main statutory agency and the scientific community \autocite{Viana.2018}. All accreditation processes for new courses and periodic evaluation are analyzed by experts and ad hoc consultants from 49 fields of knowledge \autocite{}. 




Dimensions and evaluation criteria are defined and improved periodically. The Technical-Scientific Council (CTS-ES) is responsible for preparing e approving the “Evaluation Form”. Evaluation guidelines, criteria, and procedures are, therefore, largely influenced by desired educational results. \autocite{Viana.2018}

The evaluation form sets out standards and mandatory criteria for all fields and courses. Both evaluation processes – the periodic one and the one for accreditation of new courses – must consider the requirements of the evaluation form and the guidelines established by the documents of each field of knowledge. The first standardized form was elaborated in 1998. In 2007, the CTC-ES decided to reduce the number of requirements (from 7 to 5). In 2013, a new review was carried out. In 2018, the number of requirements and items was reduced again. The current form includes only three criteria (Program, Training, and Impact on society) and 12 items \autocite{Monteiro.2019}. Through these criteria, it is intended to analyze, among other items, (i) the quality of training and scientific production of professors and students; (ii) internationalization; (iii) innovation and knowledge transfer; and (iv) the economic, social and cultural impacts on the society \autocite{Monteiro.2019}.

 Grades 6 and 7, on the other hand, are destined for courses that have high-quality standards
In Brazil, the evaluation is predominantly external, standardized, and quantitative

The external evaluation has been essential to promoting quality in a continental country like Brazil. Despite that, it has some shortcomings. In a recent report \autocite{Faljoni-Alario.2018}, the Technical Scientific Council recognized this weakness and recommended self-evaluation to be an institutionalized, permanent and participatory process. According to this document, self-evaluation must “\textelp{} be carried out through participatory processes, based on various strategies, techniques, and instruments, generating analytical reports that point out the program´s strengths and weakness and reveal policies and actions for correction and consolidation” \autocite[19]{Faljoni-Alario.2018}.



Transparency is one of the main features of the SNPG. All guidelines, criteria, results, and information about evaluation are regularly published on the CAPES website, especially the evaluation form, field-specific documents, quadrennial evaluation regulation, and the final results of the assessment. There are two robust coleta systems for collecting and managing research and graduate education (GeoCapes and Sucupira Platform). GeoCapes offers updated information on graduate programs across the country (courses, enrollments, professors, students, grades, funding, international cooperation, etc.). It is a valuable accountability tool. Users can access graduate education data since 1995.

The second system also offers multiple functionalities. The Platform Sucupira is an online tool for collecting and updating information on each of the graduate programs. Through it, courses organize and send their information to CAPES. The quadrennial evaluation is carried out based on the data available on the Sucupira Platform \autocite{Brasil.2022, 122/2021}.